title: NPFL122, Lecture 1
class: title
# Introduction to Reinforcement Learning

## Milan Straka

### October 8, 2018

---
section: History
# History of Reinforcement Learning

_Develop goal-seeking agent trained using reward signal._

~~~
- _Optimal control_ in 1950s ‚Äì Richard Bellman

~~~
- Trial and error learning ‚Äì since 1850s
  - Law and effect ‚Äì Edward Thorndike, 1911
  - Shannon, Minsky, Clark&Farley, ‚Ä¶ ‚Äì 1950s and 1960s
  - Tsetlin, Holland, Klopf ‚Äì 1970s
  - Sutton, Barto ‚Äì since 1980s

~~~
- Arthur Samuel ‚Äì first implementation of temporal difference methods
  for playing checkers

~~~
## Notable successes
- Gerry Tesauro ‚Äì 1992, human-level Backgammon playing program trained solely by self-play

~~~
- IBM Watson in Jeopardy ‚Äì 2011

---
# History of Reinforcement Learning
## Recent successes

- Human-level video game playing (DQN) ‚Äì 2013 (2015 Nature), Mnih. et al, Deepmind

  - 29 games out of 49 comparable or better to professional game players
  - 8 days on GPU
  - human-normalized mean: 121.9%, median: 47.5% on 57 games

~~~
- A3C ‚Äì 2016, Mnih. et al
  - 4 days on 16-threaded CPU
  - human-normalized mean: 623.0%, median: 112.6% on 57 games

~~~
- Rainbow ‚Äì 2017
  - human-normalized median: 153%

~~~
- Impala ‚Äì Feb 2018
  - one network and set of parameters to rule them all
  - human-normalized mean: 176.9%, median: 59.7% on 57 games

~~~
- PopArt-Impala ‚Äì Sep 2018
  - human-normalized median: 110.7% on 57 games

---
# History of Reinforcement Learning
## Recent successes

- AlphaGo

  - Mar 2016 ‚Äì beat 9-dan professional player Lee Sedol

~~~
- AlphaGo Master ‚Äì Dec 2016
  - beat 60 professionals
  - beat Ke Jie in May 2017
~~~
- AlphaGo Zero ‚Äì 2017
  - trained only using self-play
  - surpassed all previous version after 40 days of training
~~~
- AlphaZero ‚Äì Dec 2017
  - self-play only
  - defeated AlphaGo Zero after 34 hours of training (21 million games)
~~~
  - impressive chess and shogi performance after 9h and 12h, respectively

---
# History of Reinforcement Learning
## Recent successes

- Dota2 ‚Äì Aug 2017

  - won 1v1 matches against a professional player

~~~
- MERLIN ‚Äì Mar 2018
  - unsupervised representation of states using external memory
  - partial observations
  - beat human in unknown maze navigation

~~~
- FTW ‚Äì Jul 2018
  - beat professional players in two-player-team Capture the flag FPS
  - solely by self-play
  - trained on 450k games
    - each 5 minutes, 4500 agent steps (15 per second)

~~~
- OpenAI Five ‚Äì Aug 2018
  - won 5v5 best-of-three match against professional team
  - 256 GPUs, 128k CPUs
    - 180 years of experience per day

---
# History of Reinforcement Learning
## Recent successes

- Improved translation quality in 2016

~~~
- Discovering discrete latent structures

~~~
- TARDIS ‚Äì Jan 2017
  - allow using discrete external memory

‚Ä¶

---
section: Multi-armed Bandits
# Multi-armed Bandits

![w=50%,h=center,v=middle](one-armed-bandit.jpg)

---
class: middle
# Multi-armed Bandits

![w=70%,h=center,v=middle](k-armed_bandits.pdf)

---
# Multi-armed Bandits

Let $q_*(a)$ be the real _value_ of an action $a$:
$$q_*(a) = ùîº[R_{t+1} | A_t = a].$$

~~~
Denoting $Q_t(a)$ our estimated value of action $a$ at time $t$, we would like
$Q_t(a)$ to converge to $q_*(a)$.

~~~
A natural way to estimate $Q_t(a)$ is
$$Q_t(a) ‚âù \frac{\textrm{sum of rewards when action }a\textrm{ is taken}}{\textrm{number of times action }a\textrm{ was taken}}.$$

~~~
Following the definition of $Q_t(a)$, we could choose a _greedy action_ $A_t$ as
$$A_t(a) ‚âù \argmax_a Q_t(a).$$

---
# Multi-armed Bandits

## Exploitation versus Exploration

Choosing a greedy action is _exploitation_ of current estimates. We however also
need to _explore_ the space of actions to improve our estimates.

~~~

An _$Œµ$-greedy_ method follows the greedy action with probability $1-Œµ$, and
chooses a uniformly random action with probability $Œµ$.

---
# Multi-armed Bandits

![w=52%,h=center,v=middle](e_greedy.pdf)

---
# Multi-armed Bandits

## Incremental Implementation

Let $Q_n$ be an estimate using $n$ rewards $R_1, \ldots, R_n$.

$$\begin{aligned}
Q_n &= \frac{1}{n} ‚àë_{i=1}^n R_i \\
    &= \frac{1}{n} (R_n + \frac{n-1}{n-1} ‚àë_{i=1}^{n-1} R_i) \\
    &= \frac{1}{n} (R_n + (n-1) Q_{n-1}) \\
    &= \frac{1}{n} (R_n + n Q_{n-1} - Q_{n-1}) \\
    &= Q_{n-1} + \frac{1}{n}\Big(R_n - Q_{n-1}\Big)
\end{aligned}$$

---
# Multi-armed Bandits

![w=100%,v=middle](bandits_algorithm.pdf)

---
# Multi-armed Bandits

## Non-stationary Problems

Analogously to the solution obtained for a stationary problem, we consider
$$Q_{n+1} = Q_n + Œ±(R_{n+1} - Q_n).$$

~~~
Converges to the true action values if
$$‚àë_{n=1}^‚àû Œ±_n = ‚àû \textrm{~~~~and~~~~}‚àë_{n=1}^‚àû Œ±_n^2 < ‚àû.$$

~~~
Biased method, because
$$Q_{n+1} = (1 - Œ±)^n Q_1 + ‚àë_{i=1}^n Œ±(1-Œ±)^{n-i} R_i.$$

---
# Multi-armed Bandits

## Optimistic Initial Values

![w=85%,h=center,v=middle](optimistic_values.pdf)

---
# Multi-armed Bandits

## Upper Confidence Bound

$$A_t ‚âù \argmax_a \left[Q_t(a) + c\sqrt{\frac{\ln t}{N_t(a)}}\right].$$

~~~
![w=70%,h=center](ucb.pdf)

---
# Multi-armed Bandits

## Gradient Bandit Algorithms

Let $H_t(a)$ be a numerical _preference_ for an action $a$ at time $t$.

~~~
We could choose actions according to softmax distribution:
$$œÄ(A_t = a) ‚âù \frac{e^{H_t(a)}}{‚àë_b e^{H_t(b)}}.$$

~~~
Using SGD and MLE loss, we can derive the following algorithm:
$$\begin{aligned}
  H_{t+1}(A_t) &‚Üê H_t(A_t) + Œ±R_{t+1}(1 - œÄ(A_t)), \\
  H_{t+1}(a) &‚Üê H_t(a) - Œ±R_{t+1}œÄ(a) \textrm{~~~~for~~}a‚â†A_t.
\end{aligned}$$

---
# Multi-armed Bandits

## Gradient Bandit Algorithms

![w=85%,h=center,v=middle](gradient_bandits.pdf)

---
# Multi-armed Bandits

## Method Comparison

![w=80%,h=center,v=middle](bandits_comparison.pdf)
