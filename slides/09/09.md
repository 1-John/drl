title: NPFL122, Lecture 9
class: title, langtech, cc-by-nc-sa
# Deterministic Policy Gradient, Advanced RL Algorithms

## Milan Straka

### December 3, 2018

---
section: Refresh
# REINFORCE with Baseline

The returns can be arbitrary – better-than-average and worse-than-average
returns cannot be recognized from the absolute value of the return.

Hopefully, we can generalize the policy gradient theorem using a baseline $b(s)$
to
$$∇_→θ J(→θ) ∝ ∑_{s∈𝓢} μ(s) ∑_{a ∈ 𝓐} \big(q_π(s, a) - b(s)\big) ∇_→θ π(a | s; →θ).$$

A good choice for $b(s)$ is $v_π(s)$, which can be shown to minimize variance of
the estimator. Such baseline reminds centering of returns, given that
$v_π(s) = 𝔼_{a ∼ π} q_π(s, a)$. Then, better-than-average returns are positive
and worse-than-average returns are negative.

The resulting value is also called an _advantage function_
$a_π(s, a) ≝ q_π(s, a) - v_π(s)$.

Of course, the $v_π(s)$ baseline can be only approximated. If neural networks
are used to estimate $π(a|s; →θ)$, then some part of the network is usually
shared between the policy and value function estimation, which is trained using
mean square error of the predicted and observed return.

---
# Parallel Advantage Actor Critic

An alternative to independent workers is to train in a synchronous and
centralized way by having the workes to only generate episodes. Such approach
was described in May 2017 by Celemente et al., who named their agent
_parallel advantage actor-critic_ (PAAC).

![w=70%,h=center](../08/paac_framework.pdf)

---
# Continuous Action Space

Until now, the actions were discreet. However, many environments naturally
accept actions from continuous space. We now consider actions which come
from range $[a, b]$ for $a, b ∈ ℝ$, or more generally from a Cartesian product
of several such ranges:
$$∏_i [a_i, b_i].$$

![w=40%,f=right](../08/normal_distribution.pdf)
A simple way how to parametrize the action distribution is to choose them from
the normal distribution.

Given mean $μ$ and variance $σ^2$, probability density function of $𝓝(μ, σ^2)$
is
$$p(x) ≝ \frac{1}{\sqrt{2 π σ^2}} e^{\large-\frac{(x - μ)^2}{2σ^2}}.$$

---
# Continuous Action Space in Gradient Methods

Utilizing continuous action spaces in gradient-based methods is straightforward.
Instead of the $\softmax$ distribution we suitably parametrize the action value,
usually using the normal distribution. Considering only one real-valued action,
we therefore have
$$π(a | s; →θ) ≝ P\Big(a ∼ 𝓝\big(μ(s; →θ), σ(s; →θ)^2\big)\Big),$$
where $μ(s; →θ)$ and $σ(s; →θ)$ are function approximation of mean and standard
deviation of the action distribution.

The mean and standard deviation are usually computed from the shared
representation, with
- the mean being computed as a regular regression (i.e., one output neuron
  without activation);
- the standard variance (which must be positive) being computed again as
  a regression, followed most commonly by either $\exp$ or
  $\operatorname{softplus}$, where $\operatorname{softplus}(x) ≝ \log(1 + e^x)$.

---
# Continuous Action Space in Gradient Methods

During training, we compute $μ(s; →θ)$ and $σ(s; →θ)$ and then sample the action
value (clipping it to $[a, b]$ if required). To compute the loss, we utilize
the probability density function of the normal distribution (and usually also
add the entropy penalty).

```python
  mu = tf.layers.dense(hidden_layer, 1)[:, 0]
  sd = tf.layers.dense(hidden_layer, 1)[:, 0]
  sd = tf.exp(log_sd)   # or sd = tf.nn.softplus(sd)

  normal_dist = tf.distributions.Normal(mu, sd)

  # - return * log π(a|s) - entropy_regularization
  loss = - normal_dist.log_prob(self.actions) * self.returns \
         - args.entropy_regularization * normal_dist.entropy()
```

---
section: DPG
# Deterministic Policy Gradient Theorem

Combining continuous actions and Deep Q Networks is not straightforward.
In order to do so, we need a different variant of the policy gradient theorem.

~~~
Recall that in policy gradient theorem,
$$∇_→θ J(→θ) ∝ ∑_{s∈𝓢} μ(s) ∑_{a ∈ 𝓐} q_π(s, a) ∇_→θ π(a | s; →θ).$$

~~~
Now assume that the policy $π(s; →θ)$ is deterministic and computes
an action $a∈ℝ$, and that the MDP reward is also deterministic.
Then under several assumptions about continuousness (A.1 of the below
paper), we have the following _Deterministic Policy Gradient Theorem_:
$$∇_→θ J(→θ) ∝ 𝔼_{s∼μ(s)} \Big[∇_a q_π(s, a) ∇_→θ π(s; →θ) \big|_{a=π(s;→θ)}\Big].$$

The theorem was first proven in the paper Deterministic Policy Gradient Algorithms
by David Silver et al.

---
# Deterministic Policy Gradient Theorem – Proof

The proof is very similar to the original (stochastic) policy gradient theorem.
We assume that $p(s' | s, a)$, $∇_a p(s' | s, a)$, $r(s, a)$, $∇_a r(s, a)$,
$π(s; →θ)$, $∇_→θ π(s; →θ)$ are continuous in all params.

~~~
$\displaystyle ∇_→θ v_π(s) = ∇_→θ q_π(s, π(s; →θ))$

~~~
$\displaystyle \phantom{∇_→θ v_π(s)} = ∇_→θ\Big(r\big(s, π(s; →θ)\big) + γ ∫_{s'} p\big(s' | s, π(s; →θ)\big) v_π(s') \d s'\Big)$

~~~
$\displaystyle \phantom{∇_→θ v_π(s)} = ∇_→θ π(s; →θ) ∇_a r(s, a) \big|_{a=π(s; →θ)} + γ ∇_→θ ∫_{s'} p\big(s' | s, π(s; →θ)\big) v_π(s') \d s'\Big)$

~~~
$\displaystyle \phantom{∇_→θ v_π(s)} = ∇_→θ π(s; →θ) ∇_a \Big( r(s, a) \big|_{a=π(s; →θ)} + γ ∫_{s'} p\big(s' | s, a)\big) v_π(s') \d s' \Big) \\
                    \qquad\qquad\qquad + γ ∫_{s'} p\big(s' | s, π(s; →θ)\big) ∇_→θ v_π(s') \d s'$

~~~
$\displaystyle \phantom{∇_→θ v_π(s)} = ∇_→θ π(s; →θ) ∇_a q_π(s, a)\big|_{a=π(s; →θ)} + γ ∫_{s'} p\big(s' | s, π(s; →θ)\big) ∇_→θ v_π(s') \d s'$

~~~
Similarly to the gradient theorem, we finish the proof by continually expanding $∇_→θ v_π(s')$.

---
section: DDPG
# Deep Deterministic Policy Gradients

Note that the formulation of deterministic policy gradient theorem allows an
off-policy algorithm, because the loss functions no longer depends on actions
(similarly to how expected sarsa is also an off-policy algorithm).

~~~
We therefore train function approximation for both $π(s; →θ)$ and $q(s, a; →θ)$,
training $q(s, a; →θ)$ using deterministic variant of Bellman equation:
$$q(s_t, a_t; →θ) = 𝔼_{r_{t+1}, s_{t+1}} \big[r_{t+1} + γ q(s_{t+1}, μ(s_{t+1}; →θ))\big]$$
and $π(s; →θ)$ using the deterministic policy gradient.

~~~
The algorithm was first described in the paper Continuous Control with Deep Reinforcement Learning
by Timothy P. Lillicrap et al. (2015).

The authors utilize a replay buffer, target network, batch normalization for CNNs,
and perform exploration by adding a normal-distributed noise to predicted
actions.

---
# Deep Deterministic Policy Gradients

![w=65%,h=center](ddpg.pdf)

---
# Deep Deterministic Policy Gradients

![w=100%](ddpg_ablation.pdf)

---
# Deep Deterministic Policy Gradients

Results using low-dimensional (_lowd_) version of the environment, pixel representation
(_pix_) and DPG reference (_cntrl_).

![w=57%,h=center](ddpg_results.pdf)

---
section: NPG
# Natural Policy Gradient

---
section: TRPO
# Trust Region Policy Gradient

---
section: PPO
# Proximal Policy Optimization

---
section: T3D
# Twin Delayed Deep Deterministic Policy Gradient

---
section: SAC
# Soft Actor Critic
