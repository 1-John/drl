title: NPFL122, Lecture 3
class: title
# Temporal Difference Methods, Offline Metods

## Milan Straka

### October 22, 2018

---
# Action-values and Afterstates

The reason we estimate _action-value_ function $q$ is that the policy is
defined as
$$\begin{aligned}
  π(s) &≝ \argmax_a q_π(s, a) \\
       &= \argmax_a ∑\nolimits_{s', r} p(s', r | s, a) \left[r + γ v_π(s')\right]
\end{aligned}$$
and the latter form might be impossible to evaluate if we do not have the model
of the environment.

~~~
![w=80%,mw=40%,h=center,f=right](afterstates.pdf)
However, if the environment is known, it might be better to estimate returns only
for states, and there can be substantially less states than state-action pairs.

---
section: TD Methods
# TD Methods

Temporal-difference methods estimate action-value returns using one iteration of
Bellman equation instead of complete episode return.

~~~
Compared to Monte Carlo method with constant learning rate $α$, which performs
$$v(S_t) ← v(S_t) + α\left[G_t - v(S_t)\right],$$
the simplest temporal-difference method computes the following:
$$v(S_t) ← v(S_t) + α\left[R_{t+1} + γv(S_{t+1}) - v(S_t)\right],$$

---
# TD Methods

![w=70%,h=center](td_example.pdf)

~~~
![w=70%,h=center](td_example_update.pdf)

---
# TD and MC Comparison

![w=80%,h=center](td_mc_comparison_example.pdf)

~~~
![w=80%,h=center](td_mc_comparison.pdf)

---
# Sarsa

A straightforward application to the temporal-difference policy evaluation
is Sarsa algorithm, which after generating $S_t, A_t, R_{t+1}, S_{t+1}, A_{t+1}$
computes
$$q(S_t, A_t) ← q(S_t, A_t) + α\left[R_{t+1} + γ q(S_{t+1}, A_{t+1}) -q(S_t, A_t)\right].$$

~~~
![w=75%,h=center](sarsa.pdf)

---
# Expected Sarsa

The action $A_{t+1}$ is a source of variance, moving only _in expectation_.

~~~
We could improve the algorithm by considering all actions proportionally to their
policy probability, obtaining Expected Sarsa algorithm:
$$q(S_t, A_t) ← q(S_t, A_t) + α\left[R_{t+1} + γ ∑\nolimits_a π(a|S_{t+1}) q(S_{t+1}, a) -q(S_t, A_t)\right].$$

~~~
If there is a limited number of actions, such modification usually performs better.
